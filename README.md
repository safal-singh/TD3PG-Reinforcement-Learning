# TD3PG-Reinforcement-Learning
Pytorch code for implementation of Twin Delayed Deep Deterministic Policy Gradient algorithm

Steps in the implementation:
1. ReplayBuffer class - maintains the replay memory (of size max_size) of transitions undertaken by the agent. Class consists of 2 methods - add (adds new Transition entry into Replay memory. Appends entries until the memory is full, after which new transitions replace the oldest entries.), sample (Samples a subset of Replay memory of size batch_size for the networks to train. The transition elements - state, next_state, action, reward, done - are converted to numpy array and returned). Each transition is denoted by 5 variables -
      * state - represents state of the agent at current instant.
      * next_state - state of the agent at the next instant from the current one.
      * action - action taken by agent to reach the next_state from current state.
      * reward - feedback in the form of reward granted to the agent by environment for undertaking the action at current state.
      * done - indicates whether the episode is completed (reached a point of no return, as described by the environment's rules) or not.  
The class has 2 methods -
      
2. Actor class - defines the Neural network for Actors and the forward propagation method. The network 1 input (#neurons=no. of state variables), 2 hidden (400 followed by 300 neurons) and 1 output layer (#neurons=no. of possible actions). max_action parameter in constructor is used to bound the action value after noise is added to it.
3. Critic class - defines the Neural network for 2 Critics and methods for forward propagation of both Critics, first critic separately (required for backpropagating the Actor Model). Each of the critics take the state and action as inputs, and give the Q value as output. Their network consists of 2 hidden layers, with 400, 300 neurons, in order.
4. T3D class - defined the training process. Constructor initialises 2 Actor objects and 2 Critic objects, with objects of the same class having same initialized network weights. Both the Actor and Critic classes use Adam optimizer. The select_action method takes the current state as input and return the action calculated from the forward propagation of Actor model object.
5. Train method (T3D class) - Iterates through the code for the number of iterations specified. The order of executing networks - Actor Target > Critics Target > Critics Model > Actor Model. In each iteration - 
      1. Generates a sample of size batch_size from the Replay memory and converts the 5 components of each transition in sample to Pytorch tensor, according to the processor specs (cpu/cuda).
      2. Actor Target network is forward propagated using the next_state from the batch of Transitions.
      3. batch_actions from transitions in the batch are converted to tensor and normally distributed (with mean=0, std=policy_noise), followed by clamping in the range (-noise_clip, +noise_clip), yielding in noise. This noise is added to the output action generated from Actor Target network. The noise is added to ensure that network is robust to take decisions in case of noisy inputs.
      4. next_state and next_action (generated by Actor Target and mixed with noise) are fed to Target Critics, to yield their corresponding Q values predicted for the input state and action.
      5. The minimum of the 2 Q values is taken for calculating the Target Q. This is to ensure that the networks don't overestimate Q values for actions.
      6. target_Q is calculated from the minimum Target Critic Q using Bellman equation. done variable from Transition is used to ignore next state's target Q if the episode is over.
      7. Given the state, action from each Transition, the 2 Model Critics are forward propagated to calculate respective target_Q's.
      8. The loss for Model Critics is calculated as the sum of the Mean Squared Error loss for the 2 Model Critic networks with respect to the target_Q calculated using Target networks.
      9. Model Critic networks are backpropagated using this calculated loss.                                                                                        
